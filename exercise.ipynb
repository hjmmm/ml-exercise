{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "476c1e1e",
   "metadata": {},
   "source": [
    "# ReLU Activation Function in Machine Learning\n",
    "\n",
    "The Rectified Linear Unit (ReLU) is a widely used activation function in neural networks, especially in deep learning. Simply put, it's a function that outputs the input directly if the input is positive, but outputs zero if the input is zero or negative. Mathematically, it's defined as:\n",
    "\n",
    "$$\n",
    "  f(x) = max(0,x)\n",
    "$$\n",
    "\n",
    "This creates a simple \"switch\" where only positive information is passed through, while negative information is effectively turned off. Here is how the function looks:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./images/relu.png\" height=\"300px\" alt=\"Graph showing the shape of the ReLU activation function\" />\n",
    "</p>\n",
    "\n",
    "ReLU's relevance in machine learning stems from its ability to introduce non-linearity into neural networks, which is crucial for learning complex patterns in data. Without non-linear activation functions, a deep neural network would essentially behave like a single linear model, severely limiting its capacity to understand intricate relationships. Furthermore, ReLU is computationally very efficient compared to older activation functions like sigmoid or tanh, as it only involves a simple comparison operation, leading to faster training times for large networks.\n",
    "\n",
    "One of the most significant advantages of ReLU is its role in mitigating the \"vanishing gradient problem.\" In deep networks using sigmoid or tanh, gradients can become extremely small as they are propagated backward through many layers, effectively stopping earlier layers from learning. ReLU's constant gradient for positive inputs (1) allows for a more stable and effective flow of gradients, enabling deeper networks to be trained successfully and achieve better performance. <sup>[1](#source)</sup>\n",
    "\n",
    "## Exercise\n",
    "\n",
    "In the following exercise you will implement the ReLU activation function. You can use the mathematical definition described above or use a conditional to generate the same effect. Once you are done execute the unit tests in the cells below to validate your result, have fun!\n",
    "\n",
    "\n",
    "<a name=\"source\">1</a>: Description generated by Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu (x):\n",
    "    # Remember you can leverage Python's built-in math function to implement ReLU or use \n",
    "    # an if statement. \n",
    "    # The goal is to return 0 when the input x is negative or zero and the value of x in any other case.\n",
    "\n",
    "    return 5 # This will fail... unless x is equal to 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf8bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an alternative version guiding the student away from using the max function.\n",
    "# It might help the student to understand the purpose of ReLU better than simply implementing the math definition.\n",
    "\n",
    "def alternative_relu (x):\n",
    "    # Replace the return values according to the definition of ReLU.\n",
    "    # Remember that the goal is to return 0 when the input x is negative or zero and the value of x in any other case.\n",
    "    if x > 0:\n",
    "        return 10    \n",
    "\n",
    "    return 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3379d4",
   "metadata": {},
   "source": [
    "## Test cases\n",
    "\n",
    "For the test cases I am using pytest along with the ipytest library to support the tests execution within the notebook. \n",
    "\n",
    "The tests are covering examples from the range where the function needs to return 0 and where it needs to return the value of x. I also called the case of the 0 value explicitly as it is the main edge case of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required modules to execute pytest inside the notebook\n",
    "# The following line is not needed if the notebook is executed using the included Dockerfile\n",
    "# %pip install pytest ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "\n",
    "ipytest.autoconfig()\n",
    "verbosity = \"-vv\"\n",
    "\n",
    "def test_relu_should_return_0_for_negative_values():\n",
    "    message = \"The result of the ReLU function for any negative value should be zero (0)\"\n",
    "    assert solution(-0.001) == 0, message\n",
    "    assert solution(-0.1) == 0, message\n",
    "    assert solution(-5) == 0, message\n",
    "    assert solution(-20) == 0, message\n",
    "\n",
    "def test_relu_should_return_0_for_zero_value():\n",
    "    assert solution(0) == 0, \"The result of the ReLU function for zero (0) value should be zero (0)\"\n",
    "\n",
    "def test_relu_should_return_x_for_positive_values():\n",
    "    message = \"The result of the ReLU function for any positive value should be the same value\"\n",
    "    assert solution(0.001) == 0.001, message\n",
    "    assert solution(0.5) == 0.5, message\n",
    "    assert solution(1) == 1, message\n",
    "    assert solution(15) == 15, message\n",
    "    assert solution(15.001) == 15.001, message\n",
    "\n",
    "solution = relu\n",
    "\n",
    "ipytest.run(verbosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e2eb4e",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "Bellow you can find 2 valid solutions, one usin ghte max function and one with an if statement. Both versions pass the proposed unit tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca1dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution using the max function\n",
    "\n",
    "def relu_solution_1(x):\n",
    "    # Remember you can leverage Python's built-in math function to implement ReLU or use \n",
    "    # an if statement. \n",
    "    # The goal is to return 0 when the input x is negative or zero and the value of x in any other case.\n",
    "\n",
    "    return max(0, x)\n",
    "\n",
    "solution = relu_solution_1\n",
    "\n",
    "ipytest.run(verbosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de196e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution with an if\n",
    "\n",
    "def relu_solution_2(x):\n",
    "    # Replace the return values according to the definition of ReLU.\n",
    "    # Remember that the goal is to return 0 when the input x is negative or zero and the value of x in any other case.\n",
    "\n",
    "    if x > 0:\n",
    "        return x\n",
    "    return 0\n",
    "\n",
    "solution = relu_solution_2\n",
    "\n",
    "ipytest.run(verbosity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaba64c",
   "metadata": {},
   "source": [
    "### LLM usage\n",
    "\n",
    "While generating this notebook I used the Gemini 2.5 flash model to generate the description from the introduction and the code to generate the graph of the ReLU function. Here are the prompts:\n",
    "\n",
    "1. Write a simple explanation of the ReLU activation function and its relevance for machine learning in less than 3 paragraphs (formatted reply in the first cell)\n",
    "2. Generate a piece of python code that will render a graph of the ReLU activation function in a jupyter notebook\n",
    "\n",
    "Here is the response to the second prompt:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def relu(x):\n",
    "\"\"\"\n",
    "The ReLU activation function.\n",
    "\"\"\"\n",
    "return np.maximum(0, x)\n",
    "\n",
    "# Generate a range of x values\n",
    "x = np.linspace(-5, 5, 100) # From -5 to 5 with 100 points\n",
    "\n",
    "# Calculate the corresponding y values using the ReLU function\n",
    "y = relu(x)\n",
    "\n",
    "# Plot the ReLU function\n",
    "plt.figure(figsize=(8, 6)) # Set the figure size\n",
    "plt.plot(x, y, label='ReLU(x) = max(0, x)', color='blue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('ReLU Activation Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('ReLU(x)')\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend()\n",
    "\n",
    "# Highlight the axes at y=0 and x=0\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
